\def\MyName{Carlos Salinas}
\def\PDFTitle{MA_598_ML_Pres}
\def\Subjects{neural networks, sigmoidal functions, universal approximators}
\def\DueBy{February 13, 2019}

\documentclass[11pt,letterpaper]{beamer}

\usepackage[sans]{preamble}

\usetheme{Rochester}
\usecolortheme{seagull}

\title[Approximation by a Sigmoidal Function] 
{Approximation by Superpositions of a Sigmoidal Function}
%\subtitle{Universal approximators}
\author[M.\ Ruby, C.\ Salinas] 
{M.\ Ruby,% \inst{1}
  \and C.\ Salinas% \inst{1}
}
\institute[Purdue University] % (optional)
{
  % \inst{1}%
  Department of Mathematics\\
  Purdue University
 }
\date[Spring 2019] % (optional)
{Purdue University Machine Learning Seminar, 2019}
\subject{universal approximators}
\begin{document}
\frame{\titlepage}

\begin{frame}{Introduction}
  We wish to approximate a function $f\colon\R^n\to\R$ by an expression of
  the form
  \begin{equation}
    \label{eq:lin-sig-aff}
    \sum_{j=1}^N \alpha_j\sigma(\bfy_j^\rmT\bfx+\theta_j),
  \end{equation}
  where $\bfy_j\in\R^n$ and $\alpha_j,\theta_j\in\R$ are fixed. This is exactly
  a neural network with one hidden layer.
\end{frame}

\begin{frame}{The Main Result}
  The main result is the following:
  \begin{theorem}
    For $\sigma$ a continuous sigmoidal function, finite sums of the form in
    Equation \eqref{eq:lin-sig-aff}, i.e.,
    \[
      \sum_{j=1}^N \alpha_j\sigma(\bfy_j^\rmT\bfx+\theta_j),
    \]
  are dense in $C(I_n)$.
  \end{theorem}
\end{frame}

\begin{frame}{Definitions}
  \begin{definition}[Discriminatory functions]
    We say $\sigma$ is \emph{discriminatory} if given a measure $\mu$ on $I_n$
    \[
      \int_{I_n}\sigma(\bfy^\rmT\bfx+\theta)\der\mu(\bfx)=0
    \]
    for all $\bfy\in\R^n$ implies $\mu=0$.
  \end{definition}
  \begin{definition}[Sigmoidal functions]
    We say that $\sigma$ is sigmoidal if
    \[
      \begin{cases}
        \sigma(t)\to 1&\text{as }t\to +\infty,\\
        \sigma(t)\to 0&\text{as }t\to -\infty.
      \end{cases}
    \]
  \end{definition}
\end{frame}

\begin{frame}
  The proof will rely on the following more general theorem:
  \begin{theorem}
    Let $\sigma$ be continuous and discriminatory. Then the set of finite sums
    of the form
    \[
      \sum_{j=1}^N \alpha_j\sigma(\bfy_j^\rmT\bfx+\theta_j),
    \]
    is dense in $C(I_n)$.
  \end{theorem}
\end{frame}

\begin{frame}[Some Functional Analysis]
  The proof of the more general theorem relies on two important results from
  Functional Analysis; the Hahn-Banach Theorem and the Riesz Representation
  theorem.

  \begin{theorem}
    lol cats 
  \end{theorem}
  A corrolary of this is what we use:

  \begin{theorem}
    lol kitty cats
  \end{theorem}

  The intuitive understanding you should gather from this is that if we have a
  linear subspace of a Banach space, we can find a nonzero linear functional which is
  zero on that linear subspace.
\end{frame}

\begin{frame}
  Now, the Riesz Representation theorem:
  \begin{theorem}
    lol kitty kitty cats
  \end{theorem}
 The intuitive understanding you should gather from this is that linear
 functionals can be represented by a ``filter'' of sorts. 
\end{frame}

\begin{frame}
  We can now begin the proof of Theorem 1:
  First, let $S$ be the set of continuous functions that we can exactly
  represent with our neural network. Clearly, $S$ is a linear subspace of
  $C(I_N)$. So, by Hahn-Banach, there is a nonzero bounded linear functional
  that vanishes on the closure of $S$ (the closure of $S$ is the set of
  functions we can approximate with our neural network). 
\end{frame}

\begin{frame}
  Now, this linear functional has the form
  \[
L(h) = \int_{I_n} h(x) d\mu(x)
\]

But $\sigma(y^T x + \theta)$ is in $S$ for all $y$ and $\theta$. So,
\[
\int \sigma(y^Tx + \theta) d \mu(x) = 0
\]
 for all $y$ and $\theta$. But this means that $\mu$ is identically zero,
 because $\sigma$ was discriminatory. That is, our linear functional is zero
 everywhere, which is a contradiction!
\end{frame}

\begin{frame}
  Next we need to show that sigmoidal functions are in fact discriminatory.
  \begin{lemma}
    Any bounded, measurable sigmoidal function is discriminatory. In
    particular,any continuous sigmoidal function is discriminatory.
  \end{lemma}
\end{frame}

\begin{frame}
  By our previous results the main theorem now follows as a corollary to theorem
  1 and lemma 2. That is, we have proven
  \begin{theorem}
    For $\sigma$ a continuous sigmoidal function, finite sums of the form in
    Equation \eqref{eq:lin-sig-aff}, i.e.,
    \[
      \sum_{j=1}^N \alpha_j\sigma(\bfy_j^\rmT\bfx+\theta_j),
    \]
  are dense in $C(I_n)$.
  \end{theorem}
\end{frame}

\begin{frame}
\end{frame}

\begin{frame}
\end{frame}
\end{document}